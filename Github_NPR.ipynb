{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdba4752-282b-44b0-909d-069f93ff9633",
      "metadata": {
        "id": "cdba4752-282b-44b0-909d-069f93ff9633"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(\"CUDA Available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To Set up GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "  print(f\"CUDA Version: {torch.version.cuda}\")"
      ],
      "metadata": {
        "id": "6if84ORdG3fk"
      },
      "id": "6if84ORdG3fk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oCAIB-xU-sUR",
      "metadata": {
        "id": "oCAIB-xU-sUR"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4eeef439",
      "metadata": {
        "id": "4eeef439"
      },
      "outputs": [],
      "source": [
        "pip install --upgrade Pillow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb4fe8f9",
      "metadata": {
        "id": "fb4fe8f9"
      },
      "outputs": [],
      "source": [
        "pip install sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "785834e2",
      "metadata": {
        "id": "785834e2"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "335f178b",
      "metadata": {
        "id": "335f178b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, T5Tokenizer, T5ForConditionalGeneration\n",
        "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, accuracy_score\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "import subprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75085536",
      "metadata": {
        "id": "75085536"
      },
      "source": [
        "# Dataset Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b480d11c-9731-4ad0-8fdf-0d1e1a420a11",
      "metadata": {
        "id": "b480d11c-9731-4ad0-8fdf-0d1e1a420a11"
      },
      "outputs": [],
      "source": [
        "# Load the essays dataset\n",
        "essays_df = pd.read_csv(\"/content/drive/MyDrive/Training_Essay_Data.csv\")\n",
        "# Load the text dataset\n",
        "text_df = pd.read_csv(\"/content/drive/MyDrive/human_vs_ai_sentences.csv\")\n",
        "# Load the fake news dataset\n",
        "news_df = pd.read_csv(\"/content/drive/MyDrive/fake_news_dataset.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d91ac4b",
      "metadata": {
        "id": "0d91ac4b"
      },
      "source": [
        "# Data Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_df = text_df.dropna(subset=['text', 'generated'])\n"
      ],
      "metadata": {
        "id": "CceQ59oVKBB-"
      },
      "id": "CceQ59oVKBB-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_df['generated'] = text_df['generated'].astype(int)\n"
      ],
      "metadata": {
        "id": "2lG2qFhJKIgi"
      },
      "id": "2lG2qFhJKIgi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f85f796-99ce-4c90-a9ab-612f0978b7c4",
      "metadata": {
        "id": "3f85f796-99ce-4c90-a9ab-612f0978b7c4"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(df, text_column='text', label_column='generated', sample_size=None, max_length=512):\n",
        "    def preprocess_text(text):\n",
        "        # Remove HTML tags\n",
        "        text = BeautifulSoup(str(text), \"html.parser\").get_text()\n",
        "\n",
        "        # Remove special characters and standardize punctuation\n",
        "        text = re.sub(r'[^\\w\\s.,!?-]', '', text)\n",
        "\n",
        "        # Standardize whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "    # To Verify label consistency and remove entries with missing text or labels\n",
        "    df = df.dropna(subset=[text_column, label_column])\n",
        "\n",
        "    # To Preprocess the text\n",
        "    df[text_column] = df[text_column].apply(preprocess_text)\n",
        "\n",
        "    # To Ensure the label column contains only 0 and 1\n",
        "    df[label_column] = df[label_column].astype(int)\n",
        "\n",
        "    # Truncate or pad sequences\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token  # Set the pad token to the EOS token\n",
        "\n",
        "    def truncate_or_pad(text):\n",
        "        encoded = tokenizer.encode_plus(\n",
        "            text,\n",
        "            max_length=max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return tokenizer.decode(encoded['input_ids'][0], skip_special_tokens=True)\n",
        "\n",
        "    df[text_column] = df[text_column].apply(truncate_or_pad)\n",
        "\n",
        "    if sample_size:\n",
        "        df = df.sample(sample_size, random_state=42)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5033d9e2-28e5-4256-9d23-b3aa1dde1243",
      "metadata": {
        "id": "5033d9e2-28e5-4256-9d23-b3aa1dde1243"
      },
      "outputs": [],
      "source": [
        "# For essays dataset\n",
        "def prepare_essays_dataset(essays_df, sample_size=None, max_length=512):\n",
        "    return prepare_dataset(essays_df, sample_size=sample_size, max_length=max_length)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51d81454-c55e-44cb-a82d-d2f23f4fe243",
      "metadata": {
        "id": "51d81454-c55e-44cb-a82d-d2f23f4fe243"
      },
      "outputs": [],
      "source": [
        "#prepared essay dataset\n",
        "processed_essays_data = prepare_essays_dataset(essays_df, sample_size=1000, max_length=512)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count class distribution\n",
        "label_counts = processed_essays_data['generated'].value_counts()\n",
        "\n",
        "print(\"Class distribution in processed_essays_data:\")\n",
        "print(label_counts)\n",
        "\n",
        "# Optional: Check balance ratio\n",
        "ratio = label_counts.min() / label_counts.max()\n",
        "print(f\"Balance ratio: {ratio:.2f}\")\n"
      ],
      "metadata": {
        "id": "DHIRm3gy-EAU"
      },
      "id": "DHIRm3gy-EAU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Choose equal samples from each class\n",
        "sample_size = 512  # or any even number (max 2 Ã— 373 = 746 total possible)\n",
        "human_samples = processed_essays_data[processed_essays_data['generated'] == 0].sample(n=sample_size // 2, random_state=42)\n",
        "ai_samples = processed_essays_data[processed_essays_data['generated'] == 1].sample(n=sample_size // 2, random_state=42)\n",
        "\n",
        "# Combine and shuffle\n",
        "balanced_essays_sample = pd.concat([human_samples, ai_samples]).sample(frac=1, random_state=42)\n"
      ],
      "metadata": {
        "id": "1laTRL1Z-z20"
      },
      "id": "1laTRL1Z-z20",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count class distribution\n",
        "label_counts = balanced_essays_sample['generated'].value_counts()\n",
        "\n",
        "print(\"Class distribution in processed_essays_data:\")\n",
        "print(label_counts)\n",
        "\n",
        "# Optional: Check balance ratio\n",
        "ratio = label_counts.min() / label_counts.max()\n",
        "print(f\"Balance ratio: {ratio:.2f}\")\n"
      ],
      "metadata": {
        "id": "h50YXIXd-4vf"
      },
      "id": "h50YXIXd-4vf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d015ad0e-2fa9-4f47-befb-3360f9a70533",
      "metadata": {
        "id": "d015ad0e-2fa9-4f47-befb-3360f9a70533"
      },
      "outputs": [],
      "source": [
        "def prepare_text_dataset(text_df, sample_size=None, max_length=512):\n",
        "    return prepare_dataset(text_df, sample_size=sample_size, max_length=max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2c7025a-2d86-4d9d-a12d-a99bb8be5882",
      "metadata": {
        "id": "a2c7025a-2d86-4d9d-a12d-a99bb8be5882"
      },
      "outputs": [],
      "source": [
        "#prepared text dataset\n",
        "processed_text_data = prepare_text_dataset(text_df, sample_size=1000, max_length=512)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check distribution of labels (0 = human, 1 = AI)\n",
        "label_counts = news_df['generated'].value_counts()\n",
        "\n",
        "print(\"Class distribution:\")\n",
        "print(label_counts)\n",
        "\n",
        "# Optional: Display balance ratio\n",
        "balance_ratio = label_counts.min() / label_counts.max()\n",
        "print(f\"\\nBalance ratio: {balance_ratio:.2f}\")\n"
      ],
      "metadata": {
        "id": "JOREn8wTnXMb"
      },
      "id": "JOREn8wTnXMb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For news dataset\n",
        "def prepare_news_dataset(news_df, sample_size=None, max_length=512):\n",
        "    return prepare_dataset(news_df, sample_size=sample_size, max_length=max_length)\n",
        "\n"
      ],
      "metadata": {
        "id": "AlCHLKyfnZiF"
      },
      "id": "AlCHLKyfnZiF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prepared essay dataset\n",
        "processed_news_data = prepare_news_dataset(news_df, sample_size=1000, max_length=512)"
      ],
      "metadata": {
        "id": "0ulZVRyDnbEX"
      },
      "id": "0ulZVRyDnbEX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "04f7a97c",
      "metadata": {
        "id": "04f7a97c"
      },
      "source": [
        "# Pertubations T5 Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e6bab43",
      "metadata": {
        "id": "7e6bab43"
      },
      "outputs": [],
      "source": [
        "def generate_perturbations_batch(texts, t5_tokenizer, t5_model, device, num_perturbations=10, max_length=512, mask_ratio=0.15):\n",
        "    all_perturbed_texts = []\n",
        "\n",
        "    for text in texts:\n",
        "        inputs = t5_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_length).to(device)\n",
        "        input_ids = inputs.input_ids[0]\n",
        "        n_tokens = len(input_ids)\n",
        "\n",
        "        perturbed_texts = []\n",
        "\n",
        "        for _ in range(num_perturbations):\n",
        "            # Random mask indices (avoid first and last token)\n",
        "            mask = torch.rand(n_tokens) < mask_ratio\n",
        "            mask[0] = False\n",
        "            mask[-1] = False\n",
        "\n",
        "            # Replace masked tokens with <extra_id_0> token\n",
        "            masked_input_ids = []\n",
        "            mask_started = False\n",
        "            for idx, token_id in enumerate(input_ids):\n",
        "                if mask[idx]:\n",
        "                    if not mask_started:\n",
        "                        masked_input_ids.append(t5_tokenizer.convert_tokens_to_ids(\"<extra_id_0>\"))\n",
        "                        mask_started = True\n",
        "                else:\n",
        "                    masked_input_ids.append(token_id.item())\n",
        "                    mask_started = False\n",
        "\n",
        "            # Convert to text\n",
        "            masked_text = t5_tokenizer.decode(torch.tensor(masked_input_ids), skip_special_tokens=True)\n",
        "\n",
        "            # Generate perturbation\n",
        "            t5_inputs = t5_tokenizer(masked_text, return_tensors=\"pt\", truncation=True, max_length=max_length).to(device)\n",
        "            outputs = t5_model.generate(**t5_inputs, max_length=max_length, do_sample=True, top_p=0.9)\n",
        "            perturbed_text = t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "            perturbed_texts.append(perturbed_text)\n",
        "\n",
        "        all_perturbed_texts.append(perturbed_texts)\n",
        "\n",
        "    return all_perturbed_texts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2b874eb",
      "metadata": {
        "id": "b2b874eb"
      },
      "source": [
        "# NPR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "778f3eab-a76f-43c3-9d3e-a84584720aee",
      "metadata": {
        "id": "778f3eab-a76f-43c3-9d3e-a84584720aee"
      },
      "outputs": [],
      "source": [
        "def compute_log_ranks(texts, model, tokenizer, device, max_length=512):\n",
        "    log_ranks = []\n",
        "    for text in texts:\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_length).to(device)\n",
        "\n",
        "        if inputs.input_ids.size(1) == 0:  # no tokens\n",
        "            print(f\"Skipping empty tokenized input: {repr(text[:50])}\")\n",
        "            log_ranks.append(float('nan'))  # or skip\n",
        "            continue\n",
        "\n",
        "        input_ids = inputs.input_ids[0]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.logits.squeeze(0)\n",
        "\n",
        "        ranks = []\n",
        "        for idx, token_id in enumerate(input_ids):\n",
        "            token_logits = logits[idx]\n",
        "            probs = token_logits.softmax(dim=-1)\n",
        "            sorted_indices = probs.argsort(descending=True)\n",
        "            rank = (sorted_indices == token_id).nonzero(as_tuple=True)[0].item() + 1\n",
        "            ranks.append(rank)\n",
        "\n",
        "        log_rank = np.log(ranks).mean()\n",
        "        log_ranks.append(log_rank)\n",
        "\n",
        "    return log_ranks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5aeaef90",
      "metadata": {
        "id": "5aeaef90"
      },
      "outputs": [],
      "source": [
        "def compute_npr_batch(original_texts, perturbed_texts_list, model, tokenizer, device, max_length=512):\n",
        "    npr_scores = []\n",
        "\n",
        "    original_log_ranks = compute_log_ranks(original_texts, model, tokenizer, device, max_length)\n",
        "\n",
        "    for original_log_rank, perturbed_texts in zip(original_log_ranks, perturbed_texts_list):\n",
        "        perturbed_log_ranks = compute_log_ranks(perturbed_texts, model, tokenizer, device, max_length)\n",
        "\n",
        "        # Check for NaNs\n",
        "        if np.isnan(original_log_rank) or np.isnan(perturbed_log_ranks).any():\n",
        "            npr_scores.append(np.nan)\n",
        "        else:\n",
        "            npr = np.mean(perturbed_log_ranks) / (original_log_rank + 1e-8)\n",
        "            npr_scores.append(npr)\n",
        "\n",
        "    return npr_scores\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa758386",
      "metadata": {
        "id": "fa758386"
      },
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d0a7a2a",
      "metadata": {
        "id": "1d0a7a2a"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, accuracy_score, roc_curve\n",
        "def print_gpu_utilization():\n",
        "    nvidia_smi = \"nvidia-smi\"\n",
        "    try:\n",
        "        output = subprocess.check_output([nvidia_smi, \"--query-gpu=utilization.gpu,memory.used,memory.total\", \"--format=csv,noheader,nounits\"])\n",
        "        output = output.decode('utf-8').strip().split('\\n')\n",
        "        for line in output:\n",
        "            gpu_util, mem_used, mem_total = map(int, line.split(','))\n",
        "            print(f\"GPU Utilization: {gpu_util}%, Memory Used: {mem_used}MB / {mem_total}MB\")\n",
        "    except:\n",
        "        print(\"Unable to fetch GPU stats\")\n",
        "\n",
        "def evaluate_npr_metrics(npr_scores, labels):\n",
        "    auroc = roc_auc_score(labels, npr_scores)\n",
        "\n",
        "    # ROC curve to get best threshold\n",
        "    fpr, tpr, thresholds = roc_curve(labels, npr_scores)\n",
        "    j_scores = tpr - fpr\n",
        "    best_idx = j_scores.argmax()\n",
        "    best_threshold = thresholds[best_idx]\n",
        "\n",
        "    # Classify using the best threshold\n",
        "    predictions = (npr_scores >= best_threshold).astype(int)\n",
        "\n",
        "    precision = precision_score(labels, predictions)\n",
        "    recall = recall_score(labels, predictions)\n",
        "    f1 = f1_score(labels, predictions)\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "\n",
        "    return auroc, precision, recall, f1, accuracy, best_threshold\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_model_npr(model_name, dataset, t5_tokenizer, t5_model, num_perturbations=10, max_length=512, batch_size=32):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name, use_safetensors=True)\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    t5_model.to(device)\n",
        "\n",
        "    print(f\"Model ({model_name}) is on: {next(model.parameters()).device}\")\n",
        "    print(f\"T5 model is on: {next(t5_model.parameters()).device}\")\n",
        "    print_gpu_utilization()\n",
        "\n",
        "    texts = dataset['text'].tolist()\n",
        "    labels = dataset['generated'].tolist()\n",
        "\n",
        "    data = TensorDataset(torch.arange(len(texts)))\n",
        "    dataloader = DataLoader(data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    npr_scores = []\n",
        "    idx_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=f\"Evaluating {model_name}\"):\n",
        "            batch_idx = batch[0].tolist()\n",
        "            batch_texts = [texts[i] for i in batch_idx]\n",
        "            batch_perturbed_texts = generate_perturbations_batch(batch_texts, t5_tokenizer, t5_model, device, num_perturbations, max_length)\n",
        "            batch_npr = compute_npr_batch(batch_texts, batch_perturbed_texts, model, tokenizer, device, max_length)\n",
        "\n",
        "            npr_scores.extend(batch_npr)\n",
        "            idx_list.extend(batch_idx)\n",
        "\n",
        "    auroc, precision, recall, f1, accuracy, best_threshold = evaluate_npr_metrics(np.array(npr_scores), np.array(labels))\n",
        "\n",
        "    print(f\"AUROC: {auroc:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}, Accuracy: {accuracy:.4f}, Best Threshold: {best_threshold:.4f}\")\n",
        "\n",
        "    return auroc, precision, recall, f1, accuracy, best_threshold\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e422e264",
      "metadata": {
        "id": "e422e264"
      },
      "source": [
        "# EleutherAI small, large - 5 Pertubations , 128 Sample size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33da8dd6",
      "metadata": {
        "id": "33da8dd6"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "import pandas as pd\n",
        "import time\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained('t5-base', model_max_length=512)\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
        "\n",
        "sample_size = 128\n",
        "num_perturbations = 5\n",
        "max_length = 512\n",
        "batch_size = 32\n",
        "\n",
        "subset_models = [\n",
        "    'EleutherAI/gpt-neo-125M',\n",
        "    'EleutherAI/gpt-j-6B'\n",
        "]\n",
        "\n",
        "results = []\n",
        "start_time = time.time()\n",
        "\n",
        "for model_name in tqdm(subset_models, desc=\"Overall Progress\"):\n",
        "    print(f\"\\n=== Evaluating {model_name} \")\n",
        "\n",
        "    essays_sample = processed_essays_data.sample(sample_size, random_state=42)\n",
        "    essays_auc, essays_precision, essays_recall, essays_f1, essays_accuracy, essays_best_threshold = evaluate_model_npr(\n",
        "        model_name, essays_sample, t5_tokenizer, t5_model, num_perturbations, max_length, batch_size)\n",
        "\n",
        "    print(f\"Essays - AUROC: {essays_auc:.4f}, Precision: {essays_precision:.4f}, Recall: {essays_recall:.4f}, \"\n",
        "          f\"F1: {essays_f1:.4f}, Accuracy: {essays_accuracy:.4f}, Best Threshold: {essays_best_threshold:.4f}\")\n",
        "\n",
        "    text_sample = processed_text_data.sample(sample_size, random_state=42)\n",
        "    text_auc, text_precision, text_recall, text_f1, text_accuracy, text_best_threshold = evaluate_model_npr(\n",
        "        model_name, text_sample, t5_tokenizer, t5_model, num_perturbations, max_length, batch_size)\n",
        "\n",
        "    print(f\"Text - AUROC: {text_auc:.4f}, Precision: {text_precision:.4f}, Recall: {text_recall:.4f}, \"\n",
        "          f\"F1: {text_f1:.4f}, Accuracy: {text_accuracy:.4f}, Best Threshold: {text_best_threshold:.4f}\")\n",
        "\n",
        "    news_sample = processed_news_data.sample(sample_size, random_state=42)\n",
        "    news_auc, news_precision, news_recall, news_f1, news_accuracy, news_best_threshold = evaluate_model_npr(\n",
        "        model_name, news_sample, t5_tokenizer, t5_model, num_perturbations, max_length, batch_size)\n",
        "\n",
        "    print(f\"News - AUROC: {news_auc:.4f}, Precision: {news_precision:.4f}, Recall: {news_recall:.4f}, \"\n",
        "          f\"F1: {news_f1:.4f}, Accuracy: {news_accuracy:.4f}, Best Threshold: {news_best_threshold:.4f}\")\n",
        "\n",
        "    results.append({\n",
        "        'model': model_name,\n",
        "        'essays_auc': essays_auc,\n",
        "        'essays_precision': essays_precision,\n",
        "        'essays_recall': essays_recall,\n",
        "        'essays_f1': essays_f1,\n",
        "        'essays_accuracy': essays_accuracy,\n",
        "        'essays_best_threshold': essays_best_threshold,\n",
        "        'text_auc': text_auc,\n",
        "        'text_precision': text_precision,\n",
        "        'text_recall': text_recall,\n",
        "        'text_f1': text_f1,\n",
        "        'text_accuracy': text_accuracy,\n",
        "        'tet_best_threshold': text_best_threshold,\n",
        "        'news_auc': news_auc,\n",
        "        'news_precision': news_precision,\n",
        "        'news_recall': news_recall,\n",
        "        'news_f1': news_f1,\n",
        "        'news_accuracy': news_accuracy,\n",
        "        'news_best_threshold': news_best_threshold,\n",
        "    })\n",
        "\n",
        "    pd.DataFrame(results).to_csv(f'npr_results_{time.strftime(\"%Y%m%d-%H%M%S\")}.csv', index=False)\n",
        "    print(f\"Results saved for {model_name}\")\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"Elapsed time: {elapsed_time / 3600:.2f} hours\")\n",
        "\n",
        "# Final save\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv('npr_results_final.csv', index=False)\n",
        "print(\"\\n=== Final Results (Essays Only) ===\")\n",
        "print(results_df.to_string())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7f41c02",
      "metadata": {
        "id": "d7f41c02"
      },
      "source": [
        "# Pythia small, large - 5 Pertubations , 128 Sample size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2196f5df-9c48-4daf-9aa5-1fbf277434f1",
      "metadata": {
        "id": "2196f5df-9c48-4daf-9aa5-1fbf277434f1"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "import pandas as pd\n",
        "import time\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained('t5-base', model_max_length=512)\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
        "\n",
        "sample_size = 128\n",
        "num_perturbations = 5\n",
        "max_length = 512\n",
        "batch_size = 32\n",
        "\n",
        "subset_models = [\n",
        "    'EleutherAI/pythia-410m',\n",
        "    'EleutherAI/pythia-6.9b'\n",
        "]\n",
        "\n",
        "results = []\n",
        "start_time = time.time()\n",
        "\n",
        "for model_name in tqdm(subset_models, desc=\"Overall Progress\"):\n",
        "    print(f\"\\n=== Evaluating {model_name} \")\n",
        "\n",
        "    essays_sample = processed_essays_data.sample(sample_size, random_state=42)\n",
        "    essays_auc, essays_precision, essays_recall, essays_f1, essays_accuracy, essays_best_threshold = evaluate_model_npr(\n",
        "        model_name, essays_sample, t5_tokenizer, t5_model, num_perturbations, max_length, batch_size)\n",
        "\n",
        "    print(f\"Essays - AUROC: {essays_auc:.4f}, Precision: {essays_precision:.4f}, Recall: {essays_recall:.4f}, \"\n",
        "          f\"F1: {essays_f1:.4f}, Accuracy: {essays_accuracy:.4f}, Best Threshold: {essays_best_threshold:.4f}\")\n",
        "\n",
        "    text_sample = processed_text_data.sample(sample_size, random_state=42)\n",
        "    text_auc, text_precision, text_recall, text_f1, text_accuracy, text_best_threshold = evaluate_model_npr(\n",
        "        model_name, text_sample, t5_tokenizer, t5_model, num_perturbations, max_length, batch_size)\n",
        "\n",
        "    print(f\"Text - AUROC: {text_auc:.4f}, Precision: {text_precision:.4f}, Recall: {text_recall:.4f}, \"\n",
        "          f\"F1: {text_f1:.4f}, Accuracy: {text_accuracy:.4f}, Best Threshold: {text_best_threshold:.4f}\")\n",
        "\n",
        "    news_sample = processed_news_data.sample(sample_size, random_state=42)\n",
        "    news_auc, news_precision, news_recall, news_f1, news_accuracy, news_best_threshold = evaluate_model_npr(\n",
        "        model_name, news_sample, t5_tokenizer, t5_model, num_perturbations, max_length, batch_size)\n",
        "\n",
        "    print(f\"News - AUROC: {news_auc:.4f}, Precision: {news_precision:.4f}, Recall: {news_recall:.4f}, \"\n",
        "          f\"F1: {news_f1:.4f}, Accuracy: {news_accuracy:.4f}, Best Threshold: {news_best_threshold:.4f}\")\n",
        "\n",
        "    results.append({\n",
        "        'model': model_name,\n",
        "        'essays_auc': essays_auc,\n",
        "        'essays_precision': essays_precision,\n",
        "        'essays_recall': essays_recall,\n",
        "        'essays_f1': essays_f1,\n",
        "        'essays_accuracy': essays_accuracy,\n",
        "        'essays_best_threshold': essays_best_threshold,\n",
        "        'text_auc': text_auc,\n",
        "        'text_precision': text_precision,\n",
        "        'text_recall': text_recall,\n",
        "        'text_f1': text_f1,\n",
        "        'text_accuracy': text_accuracy,\n",
        "        'tet_best_threshold': text_best_threshold,\n",
        "        'news_auc': news_auc,\n",
        "        'news_precision': news_precision,\n",
        "        'news_recall': news_recall,\n",
        "        'news_f1': news_f1,\n",
        "        'news_accuracy': news_accuracy,\n",
        "        'news_best_threshold': news_best_threshold,\n",
        "    })\n",
        "\n",
        "    pd.DataFrame(results).to_csv(f'npr_results_{time.strftime(\"%Y%m%d-%H%M%S\")}.csv', index=False)\n",
        "    print(f\"Results saved for {model_name}\")\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"Elapsed time: {elapsed_time / 3600:.2f} hours\")\n",
        "\n",
        "# Final save\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv('npr_results_final.csv', index=False)\n",
        "print(\"\\n=== Final Results (Essays Only) ===\")\n",
        "print(results_df.to_string())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AfObGohQ0p_q",
      "metadata": {
        "id": "AfObGohQ0p_q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "890cea6e",
      "metadata": {
        "id": "890cea6e"
      },
      "source": [
        "# GPT2 small, Large - 5 Pertubations , 128 Sample size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3422b87e",
      "metadata": {
        "id": "3422b87e"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "import pandas as pd\n",
        "import time\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained('t5-base', model_max_length=512)\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
        "\n",
        "sample_size = 128\n",
        "num_perturbations = 5\n",
        "max_length = 512\n",
        "batch_size = 32\n",
        "\n",
        "subset_models = [\n",
        "    'gpt2',\n",
        "    'gpt-large'\n",
        "]\n",
        "\n",
        "results = []\n",
        "start_time = time.time()\n",
        "\n",
        "for model_name in tqdm(subset_models, desc=\"Overall Progress\"):\n",
        "    print(f\"\\n=== Evaluating {model_name} \")\n",
        "\n",
        "    essays_sample = processed_essays_data.sample(sample_size, random_state=42)\n",
        "    essays_auc, essays_precision, essays_recall, essays_f1, essays_accuracy, essays_best_threshold = evaluate_model_npr(\n",
        "        model_name, essays_sample, t5_tokenizer, t5_model, num_perturbations, max_length, batch_size)\n",
        "\n",
        "    print(f\"Essays - AUROC: {essays_auc:.4f}, Precision: {essays_precision:.4f}, Recall: {essays_recall:.4f}, \"\n",
        "          f\"F1: {essays_f1:.4f}, Accuracy: {essays_accuracy:.4f}, Best Threshold: {essays_best_threshold:.4f}\")\n",
        "\n",
        "    text_sample = processed_text_data.sample(sample_size, random_state=42)\n",
        "    text_auc, text_precision, text_recall, text_f1, text_accuracy, text_best_threshold = evaluate_model_npr(\n",
        "        model_name, text_sample, t5_tokenizer, t5_model, num_perturbations, max_length, batch_size)\n",
        "\n",
        "    print(f\"Text - AUROC: {text_auc:.4f}, Precision: {text_precision:.4f}, Recall: {text_recall:.4f}, \"\n",
        "          f\"F1: {text_f1:.4f}, Accuracy: {text_accuracy:.4f}, Best Threshold: {text_best_threshold:.4f}\")\n",
        "\n",
        "    news_sample = processed_news_data.sample(sample_size, random_state=42)\n",
        "    news_auc, news_precision, news_recall, news_f1, news_accuracy, news_best_threshold = evaluate_model_npr(\n",
        "        model_name, news_sample, t5_tokenizer, t5_model, num_perturbations, max_length, batch_size)\n",
        "\n",
        "    print(f\"News - AUROC: {news_auc:.4f}, Precision: {news_precision:.4f}, Recall: {news_recall:.4f}, \"\n",
        "          f\"F1: {news_f1:.4f}, Accuracy: {news_accuracy:.4f}, Best Threshold: {news_best_threshold:.4f}\")\n",
        "\n",
        "    results.append({\n",
        "        'model': model_name,\n",
        "        'essays_auc': essays_auc,\n",
        "        'essays_precision': essays_precision,\n",
        "        'essays_recall': essays_recall,\n",
        "        'essays_f1': essays_f1,\n",
        "        'essays_accuracy': essays_accuracy,\n",
        "        'essays_best_threshold': essays_best_threshold,\n",
        "        'text_auc': text_auc,\n",
        "        'text_precision': text_precision,\n",
        "        'text_recall': text_recall,\n",
        "        'text_f1': text_f1,\n",
        "        'text_accuracy': text_accuracy,\n",
        "        'tet_best_threshold': text_best_threshold,\n",
        "        'news_auc': news_auc,\n",
        "        'news_precision': news_precision,\n",
        "        'news_recall': news_recall,\n",
        "        'news_f1': news_f1,\n",
        "        'news_accuracy': news_accuracy,\n",
        "        'news_best_threshold': news_best_threshold,\n",
        "    })\n",
        "\n",
        "    pd.DataFrame(results).to_csv(f'npr_results_{time.strftime(\"%Y%m%d-%H%M%S\")}.csv', index=False)\n",
        "    print(f\"Results saved for {model_name}\")\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"Elapsed time: {elapsed_time / 3600:.2f} hours\")\n",
        "\n",
        "# Final save\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv('npr_results_final.csv', index=False)\n",
        "print(\"\\n=== Final Results (Essays Only) ===\")\n",
        "print(results_df.to_string())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  EleutherAI small, large - 10 Pertubations , 256 Sample size"
      ],
      "metadata": {
        "id": "SZ1AJUrvTtaD"
      },
      "id": "SZ1AJUrvTtaD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a46f5cc2",
      "metadata": {
        "id": "a46f5cc2"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "import pandas as pd\n",
        "import time\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained('t5-base', model_max_length=512)\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
        "\n",
        "sample_size = 256\n",
        "num_perturbations = 10\n",
        "max_length = 512\n",
        "batch_size = 32\n",
        "\n",
        "subset_models = [\n",
        "    'EleutherAI/gpt-neo-125M',\n",
        "    'EleutherAI/gpt-j-6B'\n",
        "]\n",
        "\n",
        "results = []\n",
        "start_time = time.time()\n",
        "\n",
        "for model_name in tqdm(subset_models, desc=\"Overall Progress\"):\n",
        "    print(f\"\\n=== Evaluating {model_name} \")\n",
        "\n",
        "    essays_sample = processed_essays_data.sample(sample_size, random_state=42)\n",
        "    essays_auc, essays_precision, essays_recall, essays_f1, essays_accuracy, essays_best_threshold = evaluate_model_npr(\n",
        "        model_name, essays_sample, t5_tokenizer, t5_model, num_perturbations, max_length, batch_size)\n",
        "\n",
        "    print(f\"Essays - AUROC: {essays_auc:.4f}, Precision: {essays_precision:.4f}, Recall: {essays_recall:.4f}, \"\n",
        "          f\"F1: {essays_f1:.4f}, Accuracy: {essays_accuracy:.4f}, Best Threshold: {essays_best_threshold:.4f}\")\n",
        "\n",
        "    text_sample = processed_text_data.sample(sample_size, random_state=42)\n",
        "    text_auc, text_precision, text_recall, text_f1, text_accuracy, text_best_threshold = evaluate_model_npr(\n",
        "        model_name, text_sample, t5_tokenizer, t5_model, num_perturbations, max_length, batch_size)\n",
        "\n",
        "    print(f\"Text - AUROC: {text_auc:.4f}, Precision: {text_precision:.4f}, Recall: {text_recall:.4f}, \"\n",
        "          f\"F1: {text_f1:.4f}, Accuracy: {text_accuracy:.4f}, Best Threshold: {text_best_threshold:.4f}\")\n",
        "\n",
        "    news_sample = processed_news_data.sample(sample_size, random_state=42)\n",
        "    news_auc, news_precision, news_recall, news_f1, news_accuracy, news_best_threshold = evaluate_model_npr(\n",
        "        model_name, news_sample, t5_tokenizer, t5_model, num_perturbations, max_length, batch_size)\n",
        "\n",
        "    print(f\"News - AUROC: {news_auc:.4f}, Precision: {news_precision:.4f}, Recall: {news_recall:.4f}, \"\n",
        "          f\"F1: {news_f1:.4f}, Accuracy: {news_accuracy:.4f}, Best Threshold: {news_best_threshold:.4f}\")\n",
        "\n",
        "    results.append({\n",
        "        'model': model_name,\n",
        "        'essays_auc': essays_auc,\n",
        "        'essays_precision': essays_precision,\n",
        "        'essays_recall': essays_recall,\n",
        "        'essays_f1': essays_f1,\n",
        "        'essays_accuracy': essays_accuracy,\n",
        "        'essays_best_threshold': essays_best_threshold,\n",
        "        'text_auc': text_auc,\n",
        "        'text_precision': text_precision,\n",
        "        'text_recall': text_recall,\n",
        "        'text_f1': text_f1,\n",
        "        'text_accuracy': text_accuracy,\n",
        "        'tet_best_threshold': text_best_threshold,\n",
        "        'news_auc': news_auc,\n",
        "        'news_precision': news_precision,\n",
        "        'news_recall': news_recall,\n",
        "        'news_f1': news_f1,\n",
        "        'news_accuracy': news_accuracy,\n",
        "        'news_best_threshold': news_best_threshold,\n",
        "    })\n",
        "\n",
        "    pd.DataFrame(results).to_csv(f'npr_results_{time.strftime(\"%Y%m%d-%H%M%S\")}.csv', index=False)\n",
        "    print(f\"Results saved for {model_name}\")\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"Elapsed time: {elapsed_time / 3600:.2f} hours\")\n",
        "\n",
        "# Final save\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv('npr_results_final.csv', index=False)\n",
        "print(\"\\n=== Final Results (Essays Only) ===\")\n",
        "print(results_df.to_string())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UCxxl453epwm",
      "metadata": {
        "id": "UCxxl453epwm"
      },
      "source": [
        "# Pythia small, large - 10 Pertubations , 256 Sample size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "z4JL1Kqxefey",
      "metadata": {
        "id": "z4JL1Kqxefey"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "import pandas as pd\n",
        "import time\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained('t5-base', model_max_length=512)\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
        "\n",
        "sample_size = 256\n",
        "num_perturbations = 10\n",
        "max_length = 512\n",
        "batch_size = 32\n",
        "\n",
        "subset_models = [\n",
        "    'EleutherAI/pythia-410m',\n",
        "    'EleutherAI/pythia-6.9b'\n",
        "]\n",
        "\n",
        "results = []\n",
        "start_time = time.time()\n",
        "\n",
        "for model_name in tqdm(subset_models, desc=\"Overall Progress\"):\n",
        "    print(f\"\\n=== Evaluating {model_name} \")\n",
        "\n",
        "    essays_sample = processed_essays_data.sample(sample_size, random_state=42)\n",
        "    essays_auc, essays_precision, essays_recall, essays_f1, essays_accuracy, essays_best_threshold = evaluate_model_npr(\n",
        "        model_name, essays_sample, t5_tokenizer, t5_model, num_perturbations, max_length, batch_size)\n",
        "\n",
        "    print(f\"Essays - AUROC: {essays_auc:.4f}, Precision: {essays_precision:.4f}, Recall: {essays_recall:.4f}, \"\n",
        "          f\"F1: {essays_f1:.4f}, Accuracy: {essays_accuracy:.4f}, Best Threshold: {essays_best_threshold:.4f}\")\n",
        "\n",
        "    text_sample = processed_text_data.sample(sample_size, random_state=42)\n",
        "    text_auc, text_precision, text_recall, text_f1, text_accuracy, text_best_threshold = evaluate_model_npr(\n",
        "        model_name, text_sample, t5_tokenizer, t5_model, num_perturbations, max_length, batch_size)\n",
        "\n",
        "    print(f\"Text - AUROC: {text_auc:.4f}, Precision: {text_precision:.4f}, Recall: {text_recall:.4f}, \"\n",
        "          f\"F1: {text_f1:.4f}, Accuracy: {text_accuracy:.4f}, Best Threshold: {text_best_threshold:.4f}\")\n",
        "\n",
        "    news_sample = processed_news_data.sample(sample_size, random_state=42)\n",
        "    news_auc, news_precision, news_recall, news_f1, news_accuracy, news_best_threshold = evaluate_model_npr(\n",
        "        model_name, news_sample, t5_tokenizer, t5_model, num_perturbations, max_length, batch_size)\n",
        "\n",
        "    print(f\"News - AUROC: {news_auc:.4f}, Precision: {news_precision:.4f}, Recall: {news_recall:.4f}, \"\n",
        "          f\"F1: {news_f1:.4f}, Accuracy: {news_accuracy:.4f}, Best Threshold: {news_best_threshold:.4f}\")\n",
        "\n",
        "    results.append({\n",
        "        'model': model_name,\n",
        "        'essays_auc': essays_auc,\n",
        "        'essays_precision': essays_precision,\n",
        "        'essays_recall': essays_recall,\n",
        "        'essays_f1': essays_f1,\n",
        "        'essays_accuracy': essays_accuracy,\n",
        "        'essays_best_threshold': essays_best_threshold,\n",
        "        'text_auc': text_auc,\n",
        "        'text_precision': text_precision,\n",
        "        'text_recall': text_recall,\n",
        "        'text_f1': text_f1,\n",
        "        'text_accuracy': text_accuracy,\n",
        "        'tet_best_threshold': text_best_threshold,\n",
        "        'news_auc': news_auc,\n",
        "        'news_precision': news_precision,\n",
        "        'news_recall': news_recall,\n",
        "        'news_f1': news_f1,\n",
        "        'news_accuracy': news_accuracy,\n",
        "        'news_best_threshold': news_best_threshold,\n",
        "    })\n",
        "\n",
        "    pd.DataFrame(results).to_csv(f'npr_results_{time.strftime(\"%Y%m%d-%H%M%S\")}.csv', index=False)\n",
        "    print(f\"Results saved for {model_name}\")\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"Elapsed time: {elapsed_time / 3600:.2f} hours\")\n",
        "\n",
        "# Final save\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv('npr_results_final.csv', index=False)\n",
        "print(\"\\n=== Final Results (Essays Only) ===\")\n",
        "print(results_df.to_string())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bade6363",
      "metadata": {
        "id": "bade6363"
      },
      "source": [
        "# GPT2  small, large- 10 Pertubations , 256 Sample size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf7e84b5",
      "metadata": {
        "id": "cf7e84b5"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "import pandas as pd\n",
        "import time\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained('t5-base', model_max_length=512)\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
        "\n",
        "sample_size = 256\n",
        "num_perturbations = 10\n",
        "max_length = 512\n",
        "batch_size = 32\n",
        "\n",
        "subset_models = [\n",
        "    'gpt2',\n",
        "    'gpt-large'\n",
        "]\n",
        "\n",
        "results = []\n",
        "start_time = time.time()\n",
        "\n",
        "for model_name in tqdm(subset_models, desc=\"Overall Progress\"):\n",
        "    print(f\"\\n=== Evaluating {model_name} \")\n",
        "\n",
        "    essays_sample = processed_essays_data.sample(sample_size, random_state=42)\n",
        "    essays_auc, essays_precision, essays_recall, essays_f1, essays_accuracy, essays_best_threshold = evaluate_model_npr(\n",
        "        model_name, essays_sample, t5_tokenizer, t5_model, num_perturbations, max_length, batch_size)\n",
        "\n",
        "    print(f\"Essays - AUROC: {essays_auc:.4f}, Precision: {essays_precision:.4f}, Recall: {essays_recall:.4f}, \"\n",
        "          f\"F1: {essays_f1:.4f}, Accuracy: {essays_accuracy:.4f}, Best Threshold: {essays_best_threshold:.4f}\")\n",
        "\n",
        "    text_sample = processed_text_data.sample(sample_size, random_state=42)\n",
        "    text_auc, text_precision, text_recall, text_f1, text_accuracy, text_best_threshold = evaluate_model_npr(\n",
        "        model_name, text_sample, t5_tokenizer, t5_model, num_perturbations, max_length, batch_size)\n",
        "\n",
        "    print(f\"Text - AUROC: {text_auc:.4f}, Precision: {text_precision:.4f}, Recall: {text_recall:.4f}, \"\n",
        "          f\"F1: {text_f1:.4f}, Accuracy: {text_accuracy:.4f}, Best Threshold: {text_best_threshold:.4f}\")\n",
        "\n",
        "    news_sample = processed_news_data.sample(sample_size, random_state=42)\n",
        "    news_auc, news_precision, news_recall, news_f1, news_accuracy, news_best_threshold = evaluate_model_npr(\n",
        "        model_name, news_sample, t5_tokenizer, t5_model, num_perturbations, max_length, batch_size)\n",
        "\n",
        "    print(f\"News - AUROC: {news_auc:.4f}, Precision: {news_precision:.4f}, Recall: {news_recall:.4f}, \"\n",
        "          f\"F1: {news_f1:.4f}, Accuracy: {news_accuracy:.4f}, Best Threshold: {news_best_threshold:.4f}\")\n",
        "\n",
        "    results.append({\n",
        "        'model': model_name,\n",
        "        'essays_auc': essays_auc,\n",
        "        'essays_precision': essays_precision,\n",
        "        'essays_recall': essays_recall,\n",
        "        'essays_f1': essays_f1,\n",
        "        'essays_accuracy': essays_accuracy,\n",
        "        'essays_best_threshold': essays_best_threshold,\n",
        "        'text_auc': text_auc,\n",
        "        'text_precision': text_precision,\n",
        "        'text_recall': text_recall,\n",
        "        'text_f1': text_f1,\n",
        "        'text_accuracy': text_accuracy,\n",
        "        'tet_best_threshold': text_best_threshold,\n",
        "        'news_auc': news_auc,\n",
        "        'news_precision': news_precision,\n",
        "        'news_recall': news_recall,\n",
        "        'news_f1': news_f1,\n",
        "        'news_accuracy': news_accuracy,\n",
        "        'news_best_threshold': news_best_threshold,\n",
        "    })\n",
        "\n",
        "    pd.DataFrame(results).to_csv(f'npr_results_{time.strftime(\"%Y%m%d-%H%M%S\")}.csv', index=False)\n",
        "    print(f\"Results saved for {model_name}\")\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"Elapsed time: {elapsed_time / 3600:.2f} hours\")\n",
        "\n",
        "# Final save\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv('npr_results_final.csv', index=False)\n",
        "print(\"\\n=== Final Results (Essays Only) ===\")\n",
        "print(results_df.to_string())\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}